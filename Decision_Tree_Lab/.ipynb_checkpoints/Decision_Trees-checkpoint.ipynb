{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMS3007 Decision Tree Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab we will be using the data from the cards which we looked at last week. An example card is shown below. The word and coloured box in the center of the card are the raw features of the data. The words in the corners of the cards are the labels for $4$ different classifications of the card. Each classification task has two classes, namely \"yes\" and \"no\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/data_example.png\"  width=\"300\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From last weeks lab we determined a set of features of the cards which we deemed to be helpful to determining how the cards are classified. These included, whether or not the word in the middle was a fruit or vegetable, if the word began in a vowel, or ended in a vowel, as well as the colour of the box. Thus, below you are given data with $5$ features (a $5$-tuple or $5$d vector), where index $0$ reflects if the word is a fruit (True or False), index $1$ reflects if the word starts with a vowel (True or False), index $2$ reflects if the word ends in a vowel (True or False), index $3$ reflects the colour of the box (indicated by numerical values from $0$ to $3$ with the range of colours being [blue,green,orange,red]), and lastly index $4$ reflects the word in the middle of the card (again represented by integer values from $0$ to $26$ with the list of words being [apple, apricot, asparagus, avocado, banana, bean, beet, blueberry, blackberry, broccoli, carrot, celery, \n",
    "cherry, cucumber, eggplant, fig, grape, lemon, lettuce, onion, orange, pea, pear, peach, plum, potato,spinach]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ete3'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-99ab2f751d6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mete3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTreeStyle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTextFace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_face_to_node\u001b[0m \u001b[0;31m# Just a library that helps draw trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ete3'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from ete3 import Tree, TreeStyle, TextFace, add_face_to_node # Just a library that helps draw trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_problem = 0 # 0, 1, 2, 3 # Determines which classification problem is worked on 0=Top Left, 1=Top Right, 2=Bottom Left 3=Bottom Right\n",
    "feature_names = np.array(['is_fruit', 'starts_vowel', 'end_vowel', 'data_colours', 'data_words']) # List of feature names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two files, 'card_data.txt' and 'card_categories.txt', which contain the data points and their $4$ respective labels. We load this data below. After that we split the data into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-9f2db866ba5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdata_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'card_data.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfull_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_string\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"full_data has \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" data points with \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"features\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Reading in the 4 different labels for each data point\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Reading in the data points\n",
    "data_file = open('card_data.txt', 'r')\n",
    "data_string = data_file.read()\n",
    "full_data = np.array(data_string.split(','))[:-1].astype(np.float64).reshape((1100,5))\n",
    "print(\"full_data has \", full_data.shape[0], \" data points with \", full_data.shape[1], \"features\")\n",
    "# Reading in the 4 different labels for each data point\n",
    "labels_file = open('card_categories.txt', 'r')\n",
    "labels_string = labels_file.read()\n",
    "full_y_values = np.array(labels_string.split(','))[:-1]\n",
    "full_y_values = np.where(full_y_values == 'True', True, False).reshape((1100,4))\n",
    "print(\"full_y_values has labels for \", full_y_values.shape[0], \" data points with \", full_y_values.shape[1], \"labels per data points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'full_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-920b63b8691b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# TODO: Use last 100 data points as test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest_y_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_y_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'full_data' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO: Use last 100 data points as test data\n",
    "test_data = full_data[:100]\n",
    "test_y_values = full_y_values[:100]\n",
    "\n",
    "\n",
    "# TODO: use first 1000 data points as training data\n",
    "data = full_data[1000:]\n",
    "y_values = full_y_values[1000:]\n",
    "# Note we aren't setting any hyper-parameters so we don't need a validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data loaded and know what each feature means and its range of values we can start making the decision tree. Firstly we will write a function to calculate the entropy of a set of data. A reminder that the formula to calculate entropy is:\n",
    "$H(p) = - \\sum_{i=1}^n p_i \\log_2 pi$ where $i$ reflects which unique value of a class is being use and $p_i$ is the probability of seeing that unique value out of any possible value for the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write a function which calculates the entropy of a set of data (in general we will only pass catergory labels into the function)\n",
    "# HINT: look at np.unique, note unique returns in ascending order or alphabetically\n",
    "def calc_entropy(data):\n",
    "    # TODO: Write the function to calculate entropy. The function must return the entropy value and an array of the unique values in the data\n",
    "    unique,counts = np.unique(data,return_counts = True)\n",
    "    set_count = np.array([unique,counts]).T\n",
    "    set_probs = set_count[:,1]/np.sum(set_count[:,1])\n",
    "    data_entropy = -np.sum(set_probs*np.log2(set_probs))\n",
    "    return([data_entropy, unique]) # returns the entropy of the data as well as the set of unique value for the data\n",
    "\n",
    "print(calc_entropy(y_values[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create the class which will construct the decision tree below. The primary step involved in constructing a decision tree is to find the feature which provides the most information gain (greatest decrease in entropy). Information Gain is calculated as: $Gain(D,F)=H(D) - \\frac{1}{|D|}\\sum_{f \\in values\\_of\\_F}|D_f| H(D_f)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class tree_node:\n",
    "    def __init__(self, data, labels, diagram_node, available_features):\n",
    "        self.data = data # holds the data which the tree node will use to find the best feature\n",
    "        self.labels = labels # holds the corresponding labels for each data point\n",
    "        self.diagram_node = diagram_node # Used for creating the tree diagram\n",
    "        self.available_features = available_features # Holds the list of names for the remaining features available at a tree node\n",
    "        self.feature_index = None # Holds the index of the feature in the list of features which provides the most information gain\n",
    "        self.feature_name = None # Holds the name of the feature which provides the most information gain\n",
    "        self.node_values = None # Holds the unique values of the feature which provides the most information gain\n",
    "        self.is_leaf = False # Reflects if the node is a leaf node\n",
    "        self.class_value = None # Is set to True or False if a node classifies a data point (it is a leaf node)\n",
    "        self.children = None # Array to hold the children node of this node\n",
    "        self.node_entropy = calc_entropy(labels[:,class_problem])[0] # Entropy value of the data entering the node\n",
    "                                    #(before being split by the feature which provides the most information gain)\n",
    "\n",
    "        # Three cases to consider when adding a node to the tree.\n",
    "        # 1. The node entropy is not 0, so the node still needs to split the data further, and there is still\n",
    "        #    an unused feature to split the data with\n",
    "        # 2. If the entropy is not 0 but we have already used all features to split the data\n",
    "        # 3. The entropy of the data coming into the node is 0 and so the node must just pick a classification\n",
    "        #    (ie: True or False)\n",
    "        # It may be helpful to first complete the class functions below to see how they work\n",
    "        if not (self.node_entropy == 0.0 or self.node_entropy == -0.0 or self.data.shape[1] == 0):\n",
    "            # TODO: Init an empty array for self.children, then find the feature for this node to split the data,\n",
    "            #       then descend the tree (add children to the current node)\n",
    "            self.children = np.array([])\n",
    "            self.find_feature()\n",
    "            self.descend_tree()\n",
    "\n",
    "        elif self.data.shape[1] == 0:\n",
    "            # TODO: Update self.is_leaf, get the unique values and their counts of the labels for this node's data,\n",
    "            #       then find the index of the unique value with the largest count\n",
    "            self.is_leaf = True\n",
    "            unique, counts = np.unique(labels[:,class_problem],return_counts=True)\n",
    "            majority_class = np.argmax(counts)\n",
    "            self.class_value = unique[majority_class]\n",
    "            self.feature_name = str(self.class_value)\n",
    "            self.diagram_node.name = self.feature_name\n",
    "        else:\n",
    "            self.is_leaf = True\n",
    "            self.class_value = labels[0,class_problem]\n",
    "            self.feature_name = str(labels[0,class_problem])\n",
    "            self.diagram_node.name = self.feature_name\n",
    "\n",
    "    # This function is used to calculate the information gain of each feature and pick the best feature\n",
    "    def find_feature(self):\n",
    "        print(\"Finding feature for new node\")\n",
    "        feature_entropies = np.zeros(self.data.shape[1])            # TODO # initialize the entropy of each feature to 0\n",
    "        info_gains = np.full((self.data.shape[1],), self.node_entropy, dtype =np.float64)                   # TODO # init the info gain for each feature to equal the\n",
    "                                        # entropy of the data coming into the done (ie: H(D) in the formula above)\n",
    "        for i in range(self.data.shape[1]): # For each feature\n",
    "            #print(\"Working on feature: \", i)\n",
    "            feature_entropies[i], _ = calc_entropy(self.labels[:,class_problem])          # TODO # calculate the entropy of the data into the node\n",
    "            feature_sub_classes = np.unique(self.data[:,i])             # TODO # find the unique values for the feature we are calculation info gain for\n",
    "            for sub_clas in feature_sub_classes :                          # TODO # for each unique value of the feature\n",
    "                sub_clas_data =  np.where(self.data[:,i]==sub_clas)[0]              # TODO # find the data points where this feature value occurs\n",
    "                data_ratio =  (1/self.data.shape[0])*sub_clas_data.shape[0]                 # TODO # calc how much of the total data has this feature value\n",
    "                sub_clas_entropy = calc_entropy(self.labels[sub_clas_data,class_problem])[0]            # TODO # calc entropy for the subset of data with the feature value\n",
    "                info_gains[i] =  info_gains[i] - data_ratio *sub_clas_entropy              # TODO # update the information gain\n",
    "        chosen_feature =np.argmax(info_gains)                      # TODO # choose feature which gives mac info gain\n",
    "        self.feature_index = chosen_feature # update features of the node class\n",
    "        self.feature_name = self.available_features[self.feature_index]\n",
    "        self.diagram_node.name = self.feature_name\n",
    "        print(\"Found feature: \", self.feature_name)\n",
    "\n",
    "    # This function is used to add nodes to the tree once the best feature to split the data is found\n",
    "    def descend_tree(self):\n",
    "        print(\"Descending tree with node entropy value: \", self.node_entropy)\n",
    "        unique, counts = np.unique(self.data[:,self.feature_index],return_counts=True)                             # TODO # Find the unique values of the chosen feature\n",
    "        self.node_values = unique # Update class values which holds the values of the feature it uses\n",
    "        for feature_value in unique:                                     # TODO # For each unqiue value the chosen feature can take\n",
    "            data_for_feature_value = np.where(self.data[:,self.feature_index]==feature_value)                  # TODO # Find data where unique value for the feature occurs\n",
    "            remaining_features = np.arange(self.data.shape[1])!=self.feature_index # This just drops the chosen feature from the list of unused feature names (NOTE useful for inference below)\n",
    "            new_child_diagram_node = self.diagram_node.add_child(name=\"Temp\") # used for making the tree diagram\n",
    "\n",
    "            # For each unique value of the chosen feature we add a new node. Some points to note\n",
    "            # First we only use the data which ass the unique value we are looking for for the feature\n",
    "            # this is found in the \"data_for_feature_value\" variable above\n",
    "            # Secondly we remove the feature we used to split the data, the unused features are found in the\n",
    "            # variable \"remaining_features\"\n",
    "            self.children = np.append(self.children,\n",
    "                tree_node(self.data[data_for_feature_value][:,remaining_features],self.labels[data_for_feature_value],\n",
    "                new_child_diagram_node, self.available_features[remaining_features]))\n",
    "\n",
    "    # This function infers (predicts) the class of a new/unseen data point. We call this on the test data points\n",
    "    def infer(self, data_point):\n",
    "        if not self.is_leaf: # if the node we are looking at is not a leaf node (can't classify the data point)\n",
    "            for i in range(self.node_values.shape[0]): # look through the set of values the node looks for\n",
    "                if self.node_values[i] == data_point[self.feature_index] :                              # TODO # to find which branch to descend down\n",
    "                    allocated_class = self.children[i].infer(data_point[np.arange(data_point.shape[0])!= self.feature_index])            # TODO # recursively run the infer function on the child node (excluding the features which have been used already, see how this was done to get \"remaining_features\" when decsending the tree above)\n",
    "                    return allocated_class        # return back up the tree\n",
    "            print(\"Error found new value, can't classify\")\n",
    "        else:\n",
    "            #print(\"Classified data point as: \", self.class_value)\n",
    "            return(self.class_value) # If it is a leaf node then we just return the classification given by the leaf node\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a class which we can use to make a decision tree we can now actually train a model on our data. In the below window we use every feature defined in our training data to construct the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = Tree()\n",
    "diagram_root= t.add_child(name=\"root\")# Used for diagram, creates tree and adds root node to tree diagram (use as third input to tree_node class)\n",
    "root = tree_node(data,y_values,diagram_root, feature_names)                # TODO # Use our class to train a decision tree on the training data\n",
    "print(t.get_ascii(show_internal=True)) # prints a diagram of the decision tree\n",
    "# The remainder of the window is use to draw the decsision tree. Note the last line can be removed to\n",
    "# avoid rendering the image as it can look quite bad with large trees. The printed ascii version can still be seen\n",
    "# in this case\n",
    "ts = TreeStyle()\n",
    "ts.show_leaf_name = False\n",
    "def my_layout(node):\n",
    "    F = TextFace(node.name, tight_text=True)\n",
    "    F.rotable = True\n",
    "    F.border.width = 0\n",
    "    F.margin_top = 5\n",
    "    F.margin_bottom = 5\n",
    "    F.margin_left = 5\n",
    "    F.margin_right = 5\n",
    "    add_face_to_node(F, node, column=0, position=\"branch-right\")\n",
    "ts.layout_fn = my_layout\n",
    "ts.mode = 'r'\n",
    "ts.arc_start = 270\n",
    "ts.arc_span = 185\n",
    "ts.draw_guiding_lines = True\n",
    "ts.scale = 100\n",
    "ts.branch_vertical_margin = 100\n",
    "ts.min_leaf_separation = 100\n",
    "ts.show_scale = False\n",
    "\n",
    "#t.render(file_name=\"%%inline\", w=500, h=500, tree_style=ts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following window is the same as the one above except that the decision tree here is trained without the last feature, the individual words category. This is to highlight the fact that decision trees tend to overfit the data. This can be see with 'class_problem=2' or 'class_problem=3'. In the cases where the data is separated by whether or not the word starts or ends in a vowel, then the data will also be separable by the individual words. This is undesirable, however, as the model has not found the general rule describing the data. The propensity of decision tree to be very specific is a reason for their ability to overfit, as, with noisy data, the model will become specific to accomodate the noise. Note how much smaller the model becomes when we remove the feature which the model used to overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note this block is just to check problem index 2 without the unique word data\n",
    "t2 = Tree() # Used for diagram, creates tree and adds root node to tree diagram (use as third input to tree_node class)\n",
    "root2 = t2.add_child(name ='root')    # TODO # Use our class to train a decision tree on the training data\n",
    "print(t2.get_ascii(show_internal=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we use our trained model above and run it on unseen data. This is the real test of how good of a model we have built! To provide contrast we also run it first on the training data to get its accuracy on this data.\n",
    "\n",
    "Since the first $3$ problems have no noise in them we would expect the models to be nearly perfect, even on unseen data (which also isn't noisy). For the last problem, however, 10% of the data labels in the training and test data have been flipped from their correct values. Thus, we can expect to see our model be incorrect roughly 10% of the time on this noisy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "count_correct = 0\n",
    "for i in range(data.shape[0]):\n",
    "    out =  root.infer(data[i])                    # TODO # infer the class on the data point\n",
    "    is_correct =(out == y_values[i,class_problem])            # TODO # check that the chosen class is the same as the true class label\n",
    "    if is_correct:\n",
    "        count_correct = count_correct + 1\n",
    "print(\"Final Training Accuracy: \", count_correct/y_values.shape[0])\n",
    "# Note test accuracy should be around 90% cause I've flipped 10% of train data labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "count_correct = 0\n",
    "for i in range(test_data.shape[0]):\n",
    "    out = root.infer(data[i])                    # TODO # infer the class on the data point\n",
    "    is_correct = (out == y_values[i,class_problem])      # TODO # check that the chosen class is the same as the true class label\n",
    "    if is_correct:\n",
    "        count_correct = count_correct + 1\n",
    "print(\"Final Test Accuracy: \", count_correct/y_values.shape[0])\n",
    "# Note test accuracy should be around 90% cause I've flipped 10% of train data labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
